#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ™ºæ…§èª²ç¨‹å½±ç‰‡ç”Ÿæˆç³»çµ±
ä½¿ç”¨èªéŸ³è­˜åˆ¥ + OCR + AI åˆ†æé€²è¡Œå…§å®¹åŒ¹é…
"""

import librosa
import whisper
import easyocr
import anthropic
import json
import os
import gc
import time
import psutil
from pathlib import Path
from moviepy import (
    ImageClip, AudioFileClip, CompositeVideoClip
)
from datetime import datetime
# from dotenv import load_dotenv  # å·²ç§»é™¤ dotenvï¼Œæ”¹ç”¨ streamlit.secrets

# è¼‰å…¥ .env æª”æ¡ˆ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

class AILectureCreator:
    def __init__(self, audio_path, slides_folder, output_path="ai_lecture_video.mp4"):
        self.audio_path = audio_path
        self.slides_folder = Path(slides_folder)
        self.output_path = output_path
        # å¾ç’°å¢ƒè®Šé‡è®€å–è¨­å®š
        self.fps = int(os.getenv('VIDEO_FPS', '25'))
        
        # è¨˜æ†¶é«”ç›£æ§
        self.memory_limit_gb = float(os.getenv('MEMORY_LIMIT_GB', '2.0'))  # é è¨­ 2GB è¨˜æ†¶é«”é™åˆ¶
        
        # OCR æ‰¹è™•ç†è¨­å®š
        self.batch_size = int(os.getenv('OCR_BATCH_SIZE', '1'))  # ä¸€æ¬¡è™•ç†ä¸€å¼µåœ–ç‰‡
        
        # é€²åº¦ä¿å­˜è·¯å¾‘
        self.progress_file = Path("ocr_progress.json")
        
        # åˆå§‹åŒ– Whisper å’Œ OCR
        print("ğŸ”§ åˆå§‹åŒ– AI æ¨¡çµ„...")
        
        # å¾ç’°å¢ƒè®Šé‡è®€å–è¨­å®š
        whisper_model_name = os.getenv('WHISPER_MODEL', 'base')
        ocr_languages = os.getenv('OCR_LANGUAGES', 'ch_tra,en')
        ocr_languages = ocr_languages.split(',') if isinstance(ocr_languages, str) else list(ocr_languages)
        
        # å»¶é²åˆå§‹åŒ– OCRï¼Œé¿å…é å…ˆä½”ç”¨è¨˜æ†¶é«”
        self.whisper_model = whisper.load_model(whisper_model_name)
        self.ocr_reader = None  # å»¶é²åˆå§‹åŒ–
        self.ocr_languages = ocr_languages
        
        # Claude (Anthropic) API è¨­å®š
        self.claude_client = None
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if api_key and api_key != 'your-api-key-here':
            self.claude_client = anthropic.Anthropic(api_key=api_key)
            print("   âœ… Claude API å·²å¾ .env è¼‰å…¥")
        else:
            print("   âš ï¸ æœªè¨­å®šæœ‰æ•ˆçš„ ANTHROPIC_API_KEYï¼Œå°‡ä½¿ç”¨æœ¬åœ°åŒ¹é…ç®—æ³•")
            print("   ğŸ’¡ è«‹åœ¨ .env æª”æ¡ˆè¨­å®šä½ çš„ Anthropic API Key")
    
    def check_memory_usage(self):
        """æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        memory_info = psutil.virtual_memory()
        used_gb = memory_info.used / (1024**3)
        available_gb = memory_info.available / (1024**3)
        
        print(f"   ğŸ“Š è¨˜æ†¶é«”ä½¿ç”¨: {used_gb:.1f}GB / {memory_info.total/(1024**3):.1f}GB (å¯ç”¨: {available_gb:.1f}GB)")
        
        if available_gb < 0.5:  # å°‘æ–¼ 0.5GB å¯ç”¨è¨˜æ†¶é«”
            print("   âš ï¸ è¨˜æ†¶é«”ä¸è¶³ï¼ŒåŸ·è¡Œåƒåœ¾å›æ”¶...")
            gc.collect()
            time.sleep(1)
            
            # å†æ¬¡æª¢æŸ¥
            memory_info = psutil.virtual_memory()
            available_gb = memory_info.available / (1024**3)
            if available_gb < 0.3:  # é‚„æ˜¯ä¸å¤ 
                raise MemoryError(f"è¨˜æ†¶é«”ä¸è¶³ï¼å¯ç”¨è¨˜æ†¶é«”: {available_gb:.1f}GB")
    
    def init_ocr_reader(self):
        """å»¶é²åˆå§‹åŒ– OCR è®€å–å™¨"""
        if self.ocr_reader is None:
            print("   ğŸ” åˆå§‹åŒ– OCR è®€å–å™¨...")
            self.check_memory_usage()
            self.ocr_reader = easyocr.Reader(self.ocr_languages, gpu=False)  # å¼·åˆ¶ä½¿ç”¨ CPU
            print("   âœ… OCR è®€å–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def save_ocr_progress(self, processed_slides):
        """ä¿å­˜ OCR é€²åº¦"""
        progress_data = {
            'timestamp': datetime.now().isoformat(),
            'processed_count': len(processed_slides),
            'slides': []
        }
        
        for slide in processed_slides:
            # è½‰æ› Path å°è±¡ç‚ºå­—ä¸²ä»¥ä¾¿ JSON åºåˆ—åŒ–
            slide_data = slide.copy()
            slide_data['slide_path'] = str(slide_data['slide_path'])
            progress_data['slides'].append(slide_data)
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    def load_ocr_progress(self):
        """è¼‰å…¥å·²ä¿å­˜çš„ OCR é€²åº¦"""
        if not self.progress_file.exists():
            return []
        
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            
            # è½‰æ›å› Path å°è±¡
            slides = []
            for slide in progress_data.get('slides', []):
                slide['slide_path'] = Path(slide['slide_path'])
                slides.append(slide)
            
            print(f"   ğŸ“‚ è¼‰å…¥å·²ä¿å­˜çš„é€²åº¦: {len(slides)} å¼µç°¡å ±")
            return slides
        except Exception as e:
            print(f"   âš ï¸ è¼‰å…¥é€²åº¦å¤±æ•—: {e}")
            return []

    def transcribe_audio_with_timestamps(self):
        """ä½¿ç”¨ Whisper è½‰æ›èªéŸ³ç‚ºæ–‡å­—ï¼ŒåŒ…å«æ™‚é–“æˆ³"""
        print("ğŸ¤ åˆ†æèªéŸ³å…§å®¹...")
        
        # æª¢æŸ¥è¨˜æ†¶é«”
        self.check_memory_usage()
        
        # ä½¿ç”¨ Whisper è½‰éŒ„éŸ³é »
        result = self.whisper_model.transcribe(
            self.audio_path,
            language='zh',  # å¯ä»¥æ”¹ç‚º 'en' æˆ– None (è‡ªå‹•æª¢æ¸¬)
            word_timestamps=True
        )
        
        # æ•´ç†è½‰éŒ„çµæœ
        segments = []
        for segment in result['segments']:
            segments.append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'].strip(),
                'words': segment.get('words', [])
            })
        
        total_duration = result.get('duration', librosa.get_duration(path=self.audio_path))
        
        print(f"   âœ… èªéŸ³è½‰éŒ„å®Œæˆï¼Œå…± {len(segments)} å€‹ç‰‡æ®µ")
        print(f"   ğŸ“ è½‰éŒ„å…§å®¹é è¦½: {result['text'][:100]}...")
        
        # æ¸…ç† Whisper æ¨¡å‹è¨˜æ†¶é«”
        del result
        gc.collect()
        
        return {
            'segments': segments,
            'full_text': ' '.join([seg['text'] for seg in segments]),
            'duration': total_duration
        }
    
    def process_single_slide_ocr(self, slide_path, slide_index):
        """è™•ç†å–®å¼µç°¡å ±çš„ OCR"""
        try:
            print(f"      ğŸ” è™•ç†ä¸­: {slide_path.name}")
            self.check_memory_usage()
            
            # æª¢æŸ¥åœ–ç‰‡æª”æ¡ˆæ˜¯å¦å­˜åœ¨ä¸”å¯è®€å–
            if not slide_path.exists():
                raise FileNotFoundError(f"åœ–ç‰‡æª”æ¡ˆä¸å­˜åœ¨: {slide_path}")
            
            # ä½¿ç”¨ PIL è™•ç†åœ–ç‰‡ä¸¦æœ€ä½³åŒ–è¨˜æ†¶é«”ä½¿ç”¨
            from PIL import Image
            
            # åˆå§‹åŒ– OCR è®€å–å™¨ï¼ˆå¦‚æœé‚„æ²’åˆå§‹åŒ–ï¼‰
            self.init_ocr_reader()
            
            extracted_text = ""
            temp_path = None
            
            try:
                with Image.open(slide_path) as img:
                    # è½‰æ›ç‚º RGB æ¨¡å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    
                    # å¤§å¹…ç¸®å°åœ–ç‰‡ä»¥ç¯€çœè¨˜æ†¶é«”ï¼ŒOCR å°å°åœ–ç‰‡ä¹Ÿå¾ˆæœ‰æ•ˆ
                    max_size = 1200  # é™ä½åˆ° 1200 åƒç´ 
                    if img.width > max_size or img.height > max_size:
                        img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                    
                    # å¦‚æœåœ–ç‰‡é‚„æ˜¯å¾ˆå¤§ï¼Œé€²ä¸€æ­¥ç¸®å°
                    if img.width * img.height > 800000:  # å¤§æ–¼ 800K åƒç´ 
                        scale_factor = (800000 / (img.width * img.height)) ** 0.5
                        new_width = int(img.width * scale_factor)
                        new_height = int(img.height * scale_factor)
                        img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)
                    
                    # ä¿å­˜æš«å­˜æª”æ¡ˆ
                    temp_path = f"/tmp/ocr_temp_{slide_index}_{int(time.time())}.jpg"
                    img.save(temp_path, "JPEG", quality=85, optimize=True)
                
                print(f"      ğŸ“ åœ–ç‰‡å¤§å°: {img.width}x{img.height}")
                
                # ä½¿ç”¨ EasyOCR è™•ç†
                result = self.ocr_reader.readtext(temp_path, paragraph=False, width_ths=0.9, height_ths=0.9)
                
                # æ•´ç†æå–çš„æ–‡å­—
                extracted_texts = []
                for detection in result:
                    if len(detection) >= 3:
                        bbox, text, confidence = detection
                        if confidence > 0.4:  # é™ä½ä¿¡å¿ƒåº¦é–€æª»
                            cleaned_text = text.strip()
                            if len(cleaned_text) > 1:  # éæ¿¾å–®å­—ç¬¦
                                extracted_texts.append(cleaned_text)
                
                extracted_text = ' '.join(extracted_texts)
                
                # æ¸…ç†æš«å­˜æª”æ¡ˆ
                if temp_path and Path(temp_path).exists():
                    Path(temp_path).unlink()
                
                # å¼·åˆ¶åƒåœ¾å›æ”¶
                del result
                gc.collect()
                
                return {
                    'slide_index': slide_index,
                    'slide_path': slide_path,
                    'slide_name': slide_path.name,
                    'extracted_text': extracted_text,
                    'word_count': len(extracted_text.split()) if extracted_text else 0
                }
                
            except Exception as ocr_error:
                print(f"      âŒ OCR è™•ç†å¤±æ•—: {ocr_error}")
                return {
                    'slide_index': slide_index,
                    'slide_path': slide_path,
                    'slide_name': slide_path.name,
                    'extracted_text': '',
                    'word_count': 0
                }
            finally:
                # ç¢ºä¿æ¸…ç†æš«å­˜æª”æ¡ˆ
                if temp_path and Path(temp_path).exists():
                    try:
                        Path(temp_path).unlink()
                    except:
                        pass
                
        except Exception as e:
            print(f"      âŒ è™•ç†ç°¡å ±å¤±æ•—: {e}")
            return {
                'slide_index': slide_index,
                'slide_path': slide_path,
                'slide_name': slide_path.name,
                'extracted_text': '',
                'word_count': 0
            }
    
    def extract_text_from_slides(self):
        """ä½¿ç”¨ OCR æå–ç°¡å ±æ–‡å­— - æœ€ä½³åŒ–è¨˜æ†¶é«”ç‰ˆæœ¬"""
        print("ğŸ” åˆ†æç°¡å ±å…§å®¹...")
        
        # è¼‰å…¥å·²ä¿å­˜çš„é€²åº¦
        slides_content = self.load_ocr_progress()
        processed_files = {slide['slide_name'] for slide in slides_content}
        
        # ç²å–æ‰€æœ‰ç°¡å ±æª”æ¡ˆ
        image_extensions = ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.JPEG", "*.PNG"]
        slides = []
        for ext in image_extensions:
            slides.extend(self.slides_folder.glob(ext))
        
        slides.sort(key=lambda x: x.name)
        
        # éæ¿¾å·²è™•ç†çš„æª”æ¡ˆ
        remaining_slides = [s for s in slides if s.name not in processed_files]
        
        if processed_files:
            print(f"   ğŸ“‚ è·³éå·²è™•ç†çš„ {len(processed_files)} å¼µç°¡å ±")
        
        if not remaining_slides:
            print("   âœ… æ‰€æœ‰ç°¡å ±å·²è™•ç†å®Œæˆ")
            return slides_content
        
        print(f"   ğŸ“„ éœ€è¦è™•ç† {len(remaining_slides)} å¼µç°¡å ±")
        
        # é€ä¸€è™•ç†å‰©é¤˜çš„ç°¡å ±
        for i, slide_path in enumerate(remaining_slides):
            current_index = len(slides_content)  # ä½¿ç”¨ç•¶å‰é€²åº¦ä½œç‚ºç´¢å¼•
            print(f"   ğŸ“„ åˆ†æç°¡å ± {i+1}/{len(remaining_slides)}: {slide_path.name}")
            
            try:
                # è™•ç†å–®å¼µç°¡å ±
                slide_result = self.process_single_slide_ocr(slide_path, current_index)
                slides_content.append(slide_result)
                
                # é¡¯ç¤ºçµæœ
                if slide_result['extracted_text']:
                    print(f"      âœ… æå–æ–‡å­—: {slide_result['extracted_text'][:80]}...")
                else:
                    print(f"      âš ï¸ æœªæª¢æ¸¬åˆ°æ–‡å­—")
                
                # æ¯è™•ç†å®Œä¸€å¼µå°±ä¿å­˜é€²åº¦
                self.save_ocr_progress(slides_content)
                
                # è¨˜æ†¶é«”æª¢æŸ¥å’Œæ¸…ç†
                if (i + 1) % 2 == 0:  # æ¯è™•ç† 2 å¼µåœ–ç‰‡å°±æª¢æŸ¥ä¸€æ¬¡è¨˜æ†¶é«”
                    self.check_memory_usage()
                    time.sleep(0.5)  # ç¨å¾®æš«åœè®“ç³»çµ±å‘¼å¸
                
            except MemoryError as e:
                print(f"      âŒ è¨˜æ†¶é«”ä¸è¶³: {e}")
                print(f"      ğŸ’¾ å·²ä¿å­˜é€²åº¦åˆ°ç¬¬ {len(slides_content)} å¼µ")
                raise e
            except Exception as e:
                print(f"      âŒ è™•ç†å¤±æ•—: {e}")
                # æ·»åŠ ç©ºçµæœä»¥ç¶­æŒç´¢å¼•ä¸€è‡´æ€§
                slides_content.append({
                    'slide_index': current_index,
                    'slide_path': slide_path,
                    'slide_name': slide_path.name,
                    'extracted_text': '',
                    'word_count': 0
                })
                continue
        
        print(f"   âœ… ç°¡å ±åˆ†æå®Œæˆï¼Œå…± {len(slides_content)} å¼µ")
        
        # æ¸…ç†é€²åº¦æª”æ¡ˆ
        if self.progress_file.exists():
            self.progress_file.unlink()
            print("   ğŸ—‘ï¸ æ¸…ç†é€²åº¦æª”æ¡ˆ")
        
        # æ¸…ç† OCR è®€å–å™¨ä»¥é‡‹æ”¾è¨˜æ†¶é«”
        if self.ocr_reader is not None:
            del self.ocr_reader
            self.ocr_reader = None
            gc.collect()
            print("   ğŸ§¹ æ¸…ç† OCR è®€å–å™¨è¨˜æ†¶é«”")
        
        return slides_content
    
    def ai_content_matching(self, speech_data, slides_data):
        """ä½¿ç”¨ AI åˆ†æèªéŸ³å’Œç°¡å ±å…§å®¹çš„åŒ¹é…é—œä¿‚"""
        print("ğŸ¤– AI å…§å®¹åŒ¹é…åˆ†æ...")
        
        if not self.claude_client:
            return self.fallback_content_matching(speech_data, slides_data)
        
        try:
            # æº–å‚™æç¤ºè©
            slides_info = []
            for slide in slides_data:
                slides_info.append({
                    'index': slide['slide_index'],
                    'name': slide['slide_name'],
                    'content': slide['extracted_text'][:200]  # é™åˆ¶é•·åº¦
                })
            
            speech_segments = []
            for segment in speech_data['segments']:
                speech_segments.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text']
                })
            
            user_prompt = f"""
                ç°¡å ±å…§å®¹:
                {json.dumps(slides_info, ensure_ascii=False, indent=2)}

                èªéŸ³å…§å®¹:
                {json.dumps(speech_segments, ensure_ascii=False, indent=2)}
                """
            system_prompt = """
                ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„èª²ç¨‹å½±ç‰‡è£½ä½œå°å¹«æ‰‹ã€‚
                é€™å€‹ç°¡å ±æ˜¯æŒ‰é †åºè¨­è¨ˆçš„ï¼Œæ¯å¼µç°¡å ±å°æ‡‰ä¸€å€‹æ®µè½çš„èªéŸ³å…§å®¹ï¼Œè«‹è©³ç´°åœ°åˆ†æèªéŸ³å’Œç°¡å ±çš„å…§å®¹ï¼Œåˆ¤æ–·ä»€éº¼æ™‚å€™æ‡‰è©²ä½¿ç”¨å“ªå¼µç°¡å ±ã€‚

                ## åˆ‡æ›åŸå‰‡:
                - ç°¡å ±å¿…é ˆæŒ‰é †åºæ’­æ”¾ (0â†’1â†’2â†’3...)ï¼Œä¸å¯ä»¥è·³éæˆ–é‡è¤‡ã€‚
                - åªæœ‰ç•¶èªéŸ³å…§å®¹æ˜ç¢ºé–‹å§‹è¨è«–ä¸‹ä¸€å¼µç°¡å ±çš„ä¸»é¡Œæ™‚ï¼Œæ‰åˆ‡æ›
                - å¦‚æœèªéŸ³é‚„åœ¨å»¶çºŒç•¶å‰ç°¡å ±çš„ä¸»é¡Œï¼Œå°±ä¸è¦åˆ‡æ›
                - å°‹æ‰¾æ˜ç¢ºçš„ç« ç¯€æ¨™é¡Œã€ç·¨è™Ÿæˆ–ä¸»é¡Œè½‰æ›æ™‚æ©Ÿ
                - æœ€å¾Œä¸€å¼µç°¡å ±æŒçºŒåˆ°éŸ³é »çµæŸ
                - æ¯å¼µç°¡å ±æ‡‰è©²æœ‰è¶³å¤ çš„æ™‚é–“é¡¯ç¤ºï¼Œä¸è¦éçŸ­

                ## åˆ‡æ›æ™‚æ©ŸæŒ‡æ¨™:
                - è½åˆ°æ–°çš„ä¸»é¡Œæ¨™é¡Œ
                - èªéŸ³å…§å®¹èˆ‡ä¸‹ä¸€å¼µç°¡å ±çš„æ¨™é¡Œç¬¦åˆ
                - ç•¶å‰ä¸»é¡Œæ˜é¡¯çµæŸï¼Œé–‹å§‹è¨è«–å…¨æ–°çš„æ¦‚å¿µ

                ## è¼¸å‡ºæ ¼å¼:
                è¼¸å‡ºæ ¼å¼ç‚º JSONï¼ŒåŒ…å«æ¯å¼µç°¡å ±çš„æ™‚é–“ç¯„åœ:
                {{
                "slide_timings": [
                    {{
                        "slide_index": 1,
                        "start_time": 60.0,
                        "end_time": 120.0,
                        "reason": "æ˜ç¢ºè½åˆ°ç›¸ä¼¼çš„è§€å¿µï¼Œä¸”å…§å®¹å®Œå…¨åŒ¹é…ç°¡å ±1"
                    }}
                ]
                }}
                """

            response = self.claude_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=10000,
                temperature=0.3,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            # è§£æ AI å›æ‡‰
            ai_response = response.content[0].text
            print(f"   ğŸ” Claude å®Œæ•´å›æ‡‰:")
            print("=" * 80)
            print(ai_response)
            print("=" * 80)
            
            # æå– JSON å…§å®¹ï¼ˆè™•ç† markdown ä»£ç¢¼å¡Šæ ¼å¼ï¼‰
            import re
            
            # å˜—è©¦å¾ markdown ä»£ç¢¼å¡Šä¸­æå– JSON
            json_match = re.search(r'```json\s*(.*?)\s*```', ai_response, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
            else:
                # å¦‚æœæ²’æœ‰ markdown æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›æ‡‰
                json_content = ai_response
            
            print(f"   ğŸ“„ æå–çš„ JSON å…§å®¹:")
            print("-" * 60)
            print(json_content)
            print("-" * 60)
            
            # å˜—è©¦è§£æ JSON
            try:
                timing_data = json.loads(json_content)
                slide_timings = timing_data.get('slide_timings', [])
                
                # è½‰æ›ç‚ºåŸä¾†çš„ matches æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
                matches = []
                total_duration = speech_data['duration']
                
                for timing in slide_timings:
                    slide_index = timing['slide_index']
                    start_time = timing['start_time']
                    end_time = min(timing['end_time'], total_duration)  # ç¢ºä¿ä¸è¶…ééŸ³é »é•·åº¦
                    
                    # æ‰¾åˆ°é€™å€‹æ™‚é–“ç¯„åœå…§çš„èªéŸ³ç‰‡æ®µ
                    segments_in_range = [
                        seg for seg in speech_data['segments']
                        if seg['start'] >= start_time and seg['end'] <= end_time
                    ]
                    
                    if segments_in_range:
                        # åˆä½µé€™å€‹æ™‚é–“ç¯„åœå…§çš„æ‰€æœ‰èªéŸ³æ–‡æœ¬
                        combined_text = ' '.join([seg['text'] for seg in segments_in_range])
                    else:
                        combined_text = f"æ™‚é–“ç¯„åœ {start_time:.1f}s - {end_time:.1f}s"
                    
                    matches.append({
                        'segment_start': start_time,
                        'segment_end': end_time,
                        'segment_text': combined_text,
                        'recommended_slide': slide_index,
                        'confidence': 0.9,
                        'reason': timing.get('reason', 'æŒ‰é †åºæ’­æ”¾ç°¡å ±')
                    })
                
                print(f"   âœ… AI æ™‚é–“è»¸åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(matches)} å€‹ç°¡å ±æ®µ")
                return matches
                
            except json.JSONDecodeError as e:
                print(f"   âŒ JSON è§£æéŒ¯èª¤: {e}")
                print(f"   ğŸ“„ æå–çš„ JSON å…§å®¹:\n{json_content[:1000]}...")
                # ä¸ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼Œè€Œæ˜¯æ‹‹å‡ºéŒ¯èª¤
                raise Exception(f"Claude API å›æ‡‰æ ¼å¼éŒ¯èª¤: {e}")
                
        except Exception as e:
            print(f"   âŒ AI åŒ¹é…å¤±æ•—: {e}")
            # ä¸ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼Œç›´æ¥æ‹‹å‡ºéŒ¯èª¤
            raise e
    
    def fallback_content_matching(self, speech_data, slides_data):
        """å‚™ç”¨çš„ç°¡å–®åŒ¹é…ç®—æ³•"""
        print("   ğŸ”„ ä½¿ç”¨åŸºæ–¼æ™‚é–“çš„ç°¡å–®åŒ¹é…ç®—æ³•")
        
        segments = speech_data['segments']
        slides = slides_data
        
        matches = []
        total_duration = speech_data['duration']
        
        if not segments or not slides:
            return matches
        
        # ç°¡å–®ç­–ç•¥ï¼šæ ¹æ“šæ™‚é–“å‡å‹»åˆ†é…ç°¡å ±
        for i, segment in enumerate(segments):
            # æ ¹æ“šæ™‚é–“æ¯”ä¾‹é¸æ“‡ç°¡å ±
            progress = segment['start'] / total_duration
            slide_index = min(int(progress * len(slides)), len(slides) - 1)
            
            matches.append({
                'segment_start': segment['start'],
                'segment_end': segment['end'],
                'segment_text': segment['text'],
                'recommended_slide': slide_index,
                'confidence': 0.6,
                'reason': 'åŸºæ–¼æ™‚é–“é †åºçš„è‡ªå‹•åŒ¹é…'
            })
        
        return matches
    
    def create_timeline_from_matches(self, matches, slides_data):
        """æ ¹æ“šåŒ¹é…çµæœå»ºç«‹å½±ç‰‡æ™‚é–“è»¸"""
        print("â° ç”Ÿæˆå½±ç‰‡æ™‚é–“è»¸...")
        
        timeline = []
        
        for match in matches:
            slide_index = match['recommended_slide']
            
            if 0 <= slide_index < len(slides_data):
                slide_info = slides_data[slide_index]
                
                timeline.append({
                    'start_time': match['segment_start'],
                    'end_time': match['segment_end'],
                    'duration': match['segment_end'] - match['segment_start'],
                    'slide_path': slide_info['slide_path'],
                    'slide_name': slide_info['slide_name'],
                    'speech_text': match['segment_text'],
                    'confidence': match['confidence']
                })
        
        # åˆä½µç›¸åŒç°¡å ±çš„é€£çºŒç‰‡æ®µ
        merged_timeline = self.merge_consecutive_slides(timeline)
        
        print(f"   âœ… æ™‚é–“è»¸ç”Ÿæˆå®Œæˆï¼Œå…± {len(merged_timeline)} å€‹ç‰‡æ®µ")
        return merged_timeline
    
    def merge_consecutive_slides(self, timeline):
        """åˆä½µä½¿ç”¨ç›¸åŒç°¡å ±çš„é€£çºŒç‰‡æ®µ"""
        if not timeline:
            return []
        
        merged = []
        current_segment = timeline[0].copy()
        
        for i in range(1, len(timeline)):
            next_segment = timeline[i]
            
            # å¦‚æœæ˜¯ç›¸åŒçš„ç°¡å ±ä¸”æ™‚é–“é€£çºŒï¼Œå°±åˆä½µ
            if (current_segment['slide_path'] == next_segment['slide_path'] and 
                abs(current_segment['end_time'] - next_segment['start_time']) < 1.0):
                
                # åˆä½µç‰‡æ®µ
                current_segment['end_time'] = next_segment['end_time']
                current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                current_segment['speech_text'] += ' ' + next_segment['speech_text']
                
            else:
                # ä¸åŒç°¡å ±ï¼ŒåŠ å…¥å‰ä¸€å€‹ç‰‡æ®µä¸¦é–‹å§‹æ–°çš„
                merged.append(current_segment)
                current_segment = next_segment.copy()
        
        # åŠ å…¥æœ€å¾Œä¸€å€‹ç‰‡æ®µ
        merged.append(current_segment)
        
        return merged
    
    def create_video_clips(self, timeline):
        """æ ¹æ“šæ™‚é–“è»¸å»ºç«‹å½±ç‰‡ç‰‡æ®µ"""
        print("ğŸ¥ ç”Ÿæˆå½±ç‰‡ç‰‡æ®µ...")
        
        clips = []
        
        for i, segment in enumerate(timeline):
            try:
                slide_path = segment['slide_path']
                start_time = segment['start_time']
                duration = segment['duration']
                
                # ç¢ºä¿æœ€çŸ­æŒçºŒæ™‚é–“
                duration = max(duration, 1.0)
                
                # å»ºç«‹åœ–ç‰‡ç‰‡æ®µ
                img_clip = ImageClip(str(slide_path), duration=duration)
                img_clip = img_clip.resized(height=720)
                img_clip = img_clip.with_start(start_time)
                
                clips.append(img_clip)
                
                print(f"   ğŸ“„ ç‰‡æ®µ {i+1}: {slide_path.name} ({duration:.1f}s) - {segment['speech_text'][:50]}...")
                
            except Exception as e:
                print(f"   âš ï¸ è™•ç†ç‰‡æ®µ {i+1} æ™‚å‡ºéŒ¯: {e}")
                continue
        
        print(f"   âœ… æˆåŠŸå»ºç«‹ {len(clips)} å€‹å½±ç‰‡ç‰‡æ®µ")
        return clips
    
    def generate_smart_video(self):
        """ç”Ÿæˆæ™ºæ…§èª²ç¨‹å½±ç‰‡"""
        print("ğŸš€ é–‹å§‹ AI æ™ºæ…§å½±ç‰‡ç”Ÿæˆ...")
        print("=" * 60)
        
        # 1. èªéŸ³è½‰æ–‡å­—
        speech_data = self.transcribe_audio_with_timestamps()
        
        # 2. ç°¡å ±æ–‡å­—æå–
        slides_data = self.extract_text_from_slides()
        
        if not slides_data:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç°¡å ±ï¼")
            return
        
        # 3. AI å…§å®¹åŒ¹é…
        matches = self.ai_content_matching(speech_data, slides_data)
        
        if not matches:
            print("âŒ ç„¡æ³•ç”Ÿæˆå…§å®¹åŒ¹é…ï¼")
            return
        
        # 4. å»ºç«‹æ™‚é–“è»¸
        timeline = self.create_timeline_from_matches(matches, slides_data)
        
        # 5. ç”Ÿæˆå½±ç‰‡ç‰‡æ®µ
        video_clips = self.create_video_clips(timeline)
        
        if not video_clips:
            print("âŒ ç„¡æ³•å»ºç«‹å½±ç‰‡ç‰‡æ®µï¼")
            return
        
        # 6. åˆæˆæœ€çµ‚å½±ç‰‡
        print("ğŸ¬ åˆä½µå½±ç‰‡...")
        audio_clip = AudioFileClip(self.audio_path)
        
        # ç¢ºä¿å½±ç‰‡æ™‚é•·ä¸è¶…ééŸ³é »æ™‚é•·
        video_duration = min(max([clip.end for clip in video_clips]), audio_clip.duration)
        
        final_video = CompositeVideoClip(video_clips, size=(1280, 720))
        final_video = final_video.with_audio(audio_clip)
        final_video = final_video.with_duration(video_duration)
        
        # 7. è¼¸å‡ºå½±ç‰‡
        print(f"ğŸ’¾ æ­£åœ¨å„²å­˜å½±ç‰‡åˆ° {self.output_path}...")
        final_video.write_videofile(
            self.output_path,
            fps=self.fps,
            codec='libx264',
            audio_codec='aac'
        )
        
        print("âœ… AI æ™ºæ…§å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
        
        # 8. ç”ŸæˆåŒ¹é…å ±å‘Š
        self.generate_matching_report(matches, timeline)
        
        # æ¸…ç†è¨˜æ†¶é«”
        final_video.close()
        audio_clip.close()
        for clip in video_clips:
            clip.close()
    
    def generate_matching_report(self, matches, timeline):
        """ç”Ÿæˆå…§å®¹åŒ¹é…å ±å‘Š"""
        # å»ºç«‹ logs è³‡æ–™å¤¾ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
        logs_dir = Path("logs")
        logs_dir.mkdir(parents=True, exist_ok=True)
        # ç”¢ç”Ÿæ™‚é–“æˆ³æª”å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = logs_dir / f"matching_report_{timestamp}.json"
        
        report = {
            'generation_time': datetime.now().isoformat(),
            'total_matches': len(matches),
            'total_timeline_segments': len(timeline),
            'matches': matches,
            'timeline': []
        }
        
        # è½‰æ› timeline ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        for segment in timeline:
            report['timeline'].append({
                'start_time': segment['start_time'],
                'end_time': segment['end_time'],
                'duration': segment['duration'],
                'slide_name': segment['slide_name'],
                'speech_text': segment['speech_text'],
                'confidence': segment['confidence']
            })
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š åŒ¹é…å ±å‘Šå·²å„²å­˜è‡³: {report_path}")

def main():
    """ä¸»å‡½æ•¸ - EC2 æœ€ä½³åŒ–ç‰ˆæœ¬"""
    print("ğŸ§  AI æ™ºæ…§èª²ç¨‹å½±ç‰‡ç”Ÿæˆç³»çµ± (EC2 æœ€ä½³åŒ–ç‰ˆ)")
    print("=" * 60)
    
    # è¨­å®š matplotlib å¾Œç«¯ä»¥é¿å… GUI éŒ¯èª¤
    import matplotlib
    matplotlib.use('Agg')
    
    # æª¢æŸ¥ç³»çµ±è³‡æº
    try:
        import psutil
        memory_info = psutil.virtual_memory()
        print(f"ğŸ’¾ ç³»çµ±è¨˜æ†¶é«”: {memory_info.total/(1024**3):.1f}GB (å¯ç”¨: {memory_info.available/(1024**3):.1f}GB)")
        if memory_info.available < 1024**3:  # å°‘æ–¼ 1GB
            print("âš ï¸  è¨˜æ†¶é«”å¯èƒ½ä¸è¶³ï¼Œå»ºè­°ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³")
    except ImportError:
        print("âš ï¸  æœªå®‰è£ psutilï¼Œç„¡æ³•ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨")
    
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    audio_path = "audio.mp3"
    slides_folder = "images"
    output_path = "ai_lecture_video.mp4"
    
    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(audio_path):
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³é »æª”æ¡ˆ: {audio_path}")
        return
    
    if not os.path.exists(slides_folder):
        print(f"âŒ æ‰¾ä¸åˆ°ç°¡å ±è³‡æ–™å¤¾: {slides_folder}")
        return
    
    # æª¢æŸ¥ç°¡å ±æ•¸é‡
    image_extensions = ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.JPEG", "*.PNG"]
    slides = []
    for ext in image_extensions:
        slides.extend(Path(slides_folder).glob(ext))
    print(f"ğŸ“„ ç™¼ç¾ {len(slides)} å¼µç°¡å ±")
    
    # æª¢æŸ¥ .env æª”æ¡ˆå’Œ API Key
    if not os.path.exists('.env'):
        print("âš ï¸  æç¤º: æœªæ‰¾åˆ° .env æª”æ¡ˆï¼Œå°‡å»ºç«‹ç¯„ä¾‹æª”æ¡ˆ")
        with open('.env', 'w', encoding='utf-8') as f:
            f.write("# AI æ™ºæ…§èª²ç¨‹å½±ç‰‡ç”Ÿæˆç³»çµ±è¨­å®šæª” - EC2 æœ€ä½³åŒ–ç‰ˆ\n")
            f.write("# è«‹å°‡ your-api-key-here æ›¿æ›ç‚ºä½ çš„å¯¦éš› Anthropic API Key\n\n")
            f.write("ANTHROPIC_API_KEY=your-api-key-here\n")
            f.write("\n# EC2 æœ€ä½³åŒ–è¨­å®š\n")
            f.write("WHISPER_MODEL=base\n")
            f.write("OCR_LANGUAGES=ch_tra,en\n")
            f.write("VIDEO_FPS=25\n")
            f.write("MEMORY_LIMIT_GB=1.5\n")
            f.write("OCR_BATCH_SIZE=1\n")
            f.write("USE_GPU=false\n")
        print("   âœ… å·²å»ºç«‹ .env æª”æ¡ˆï¼Œè«‹ç·¨è¼¯ä¸¦è¨­å®šä½ çš„ API Key")
        print("   ğŸ’¡ å¯åƒè€ƒ env_example.txt æª”æ¡ˆäº†è§£å®Œæ•´è¨­å®šé¸é …")
    
    api_key = os.getenv('ANTHROPIC_API_KEY')
    if not api_key or api_key == 'your-api-key-here':
        print("âš ï¸  æç¤º: è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šæœ‰æ•ˆçš„ ANTHROPIC_API_KEY")
        print("   ç·¨è¼¯ .env æª”æ¡ˆä¸¦å°‡ 'your-api-key-here' æ›¿æ›ç‚ºå¯¦éš›çš„ API Key")
    
    # å»ºç«‹ AI å½±ç‰‡ç”Ÿæˆå™¨
    try:
        creator = AILectureCreator(audio_path, slides_folder, output_path)
        print("âœ… AI å½±ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # é¡¯ç¤ºç•¶å‰è¨­å®š
        print(f"ğŸ”§ ç•¶å‰è¨­å®š:")
        print(f"   â€¢ Whisper æ¨¡å‹: {os.getenv('WHISPER_MODEL', 'base')}")
        print(f"   â€¢ OCR èªè¨€: {os.getenv('OCR_LANGUAGES', 'ch_tra,en')}")
        print(f"   â€¢ è¨˜æ†¶é«”é™åˆ¶: {os.getenv('MEMORY_LIMIT_GB', '2.0')}GB")
        print(f"   â€¢ æ‰¹è™•ç†å¤§å°: {os.getenv('OCR_BATCH_SIZE', '1')}")
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ– AI å½±ç‰‡ç”Ÿæˆå™¨å¤±æ•—: {e}")
        import traceback
        print("è©³ç´°éŒ¯èª¤è³‡è¨Š:")
        traceback.print_exc()
        return
    
    # ç”Ÿæˆå½±ç‰‡
    try:
        print("ğŸš€ é–‹å§‹ç”Ÿæˆå½±ç‰‡...")
        start_time = time.time()
        
        creator.generate_smart_video()
        
        end_time = time.time()
        duration = end_time - start_time
        print(f"\nğŸ‰ æˆåŠŸï¼ä½ çš„ AI æ™ºæ…§å½±ç‰‡å·²å„²å­˜ç‚º: {output_path}")
        print(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {duration/60:.1f} åˆ†é˜")
        print("\nğŸš€ ç³»çµ±ç‰¹è‰²:")
        print("   â€¢ ä½¿ç”¨ Whisper é€²è¡Œç²¾ç¢ºèªéŸ³è­˜åˆ¥")
        print("   â€¢ ä½¿ç”¨ OCR æå–ç°¡å ±æ–‡å­—å…§å®¹")
        print("   â€¢ ä½¿ç”¨ Claude AI åˆ†æèªæ„ç›¸é—œæ€§é€²è¡Œæ™ºæ…§åŒ¹é…")
        print("   â€¢ è‡ªå‹•åˆä½µé€£çºŒç›¸åŒç°¡å ±ç‰‡æ®µ")
        print("   â€¢ ç”Ÿæˆè©³ç´°çš„åŒ¹é…åˆ†æå ±å‘Š")
        print("   â€¢ EC2 è¨˜æ†¶é«”æœ€ä½³åŒ–è™•ç†")
        print("   â€¢ æ”¯æ´ç¨‹å¼ä¸­æ–·å¾Œæ¢å¾©é€²åº¦")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ä½¿ç”¨è€…ä¸­æ–·è™•ç†")
        print("ğŸ’¾ å·²ä¿å­˜çš„é€²åº¦æª”æ¡ˆ: ocr_progress.json")
        print("ğŸ”„ ä¸‹æ¬¡åŸ·è¡Œæ™‚å°‡è‡ªå‹•æ¢å¾©é€²åº¦")
    except MemoryError as e:
        print(f"âŒ è¨˜æ†¶é«”ä¸è¶³: {e}")
        print("ğŸ’¡ å»ºè­°çš„è§£æ±ºæ–¹æ¡ˆ:")
        print("   â€¢ ç¸®å°ç°¡å ±åœ–ç‰‡å°ºå¯¸ (å»ºè­° < 1MB)")
        print("   â€¢ æ¸›å°‘ç°¡å ±æ•¸é‡ (åˆ†æ‰¹è™•ç†)")
        print("   â€¢ å‡ç´šåˆ°æ›´å¤§è¨˜æ†¶é«”çš„ EC2 å¯¦ä¾‹")
        print("   â€¢ èª¿æ•´ .env ä¸­çš„ MEMORY_LIMIT_GB è¨­å®š")
        print("   â€¢ è¨­å®š WHISPER_MODEL=tiny ä½¿ç”¨æ›´å°çš„æ¨¡å‹")
        print("ğŸ’¾ å·²ä¿å­˜çš„é€²åº¦æª”æ¡ˆ: ocr_progress.json")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        print("è©³ç´°éŒ¯èª¤è³‡è¨Š:")
        traceback.print_exc()
        
        # ä¿å­˜éŒ¯èª¤æ—¥èªŒ
        try:
            from datetime import datetime
            from pathlib import Path
            
            logs_dir = Path("logs")
            logs_dir.mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            error_log_path = logs_dir / f"error_log_{timestamp}.txt"
            
            with open(error_log_path, 'w', encoding='utf-8') as f:
                f.write(f"éŒ¯èª¤æ™‚é–“: {datetime.now().isoformat()}\n")
                f.write(f"ç³»çµ±è³‡è¨Š:\n")
                try:
                    memory_info = psutil.virtual_memory()
                    f.write(f"  ç¸½è¨˜æ†¶é«”: {memory_info.total/(1024**3):.1f}GB\n")
                    f.write(f"  å¯ç”¨è¨˜æ†¶é«”: {memory_info.available/(1024**3):.1f}GB\n")
                    f.write(f"  ä½¿ç”¨ç‡: {memory_info.percent}%\n")
                except:
                    f.write("  ç„¡æ³•ç²å–è¨˜æ†¶é«”è³‡è¨Š\n")
                f.write(f"\néŒ¯èª¤è¨Šæ¯: {e}\n")
                f.write("è©³ç´°éŒ¯èª¤è³‡è¨Š:\n")
                f.write(traceback.format_exc())
            
            print(f"ğŸ“ éŒ¯èª¤æ—¥èªŒå·²å„²å­˜è‡³: {error_log_path}")
            print("ğŸ’¾ å·²ä¿å­˜çš„é€²åº¦æª”æ¡ˆ: ocr_progress.json")
        except:
            pass

if __name__ == "__main__":
    main() 