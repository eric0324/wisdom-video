#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ™ºæ…§èª²ç¨‹å½±ç‰‡ç”Ÿæˆç³»çµ±
ä½¿ç”¨èªéŸ³è­˜åˆ¥ + OCR + AI åˆ†æé€²è¡Œå…§å®¹åŒ¹é…
"""

import librosa
import whisper
import easyocr
import anthropic
import json
import os
from pathlib import Path
from moviepy import (
    ImageClip, AudioFileClip, CompositeVideoClip
)
from datetime import datetime
# from dotenv import load_dotenv  # å·²ç§»é™¤ dotenvï¼Œæ”¹ç”¨ streamlit.secrets

# æ–°å¢ï¼šè‡ªå‹•åµæ¸¬ streamlit ä¸¦å–å¾— secrets
try:
    import streamlit as st
    _ST_SECRETS = st.secrets if hasattr(st, 'secrets') else None
except ImportError:
    _ST_SECRETS = None

# è¼‰å…¥ .env æª”æ¡ˆï¼ˆåƒ…æœ¬åœ°é–‹ç™¼ç”¨ï¼‰
if not _ST_SECRETS:
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ImportError:
        pass

class AILectureCreator:
    def __init__(self, audio_path, slides_folder, output_path="ai_lecture_video.mp4"):
        self.audio_path = audio_path
        self.slides_folder = Path(slides_folder)
        self.output_path = output_path
        # å„ªå…ˆå¾ streamlit secrets è®€å–è¨­å®š
        self.fps = int(
            (_ST_SECRETS.get('VIDEO_FPS') if _ST_SECRETS else os.getenv('VIDEO_FPS', '25'))
        )
        
        # åˆå§‹åŒ– Whisper å’Œ OCR
        print("ğŸ”§ åˆå§‹åŒ– AI æ¨¡çµ„...")
        
        # å¾ secrets æˆ– .env è®€å–è¨­å®š
        whisper_model_name = (_ST_SECRETS.get('WHISPER_MODEL') if _ST_SECRETS else os.getenv('WHISPER_MODEL', 'base'))
        ocr_languages = (_ST_SECRETS.get('OCR_LANGUAGES') if _ST_SECRETS else os.getenv('OCR_LANGUAGES', 'ch_tra,en'))
        ocr_languages = ocr_languages.split(',') if isinstance(ocr_languages, str) else list(ocr_languages)
        
        self.whisper_model = whisper.load_model(whisper_model_name)
        self.ocr_reader = easyocr.Reader(ocr_languages)  # æ”¯æ´ä¸­æ–‡å’Œè‹±æ–‡
        
        # Claude (Anthropic) API è¨­å®š
        self.claude_client = None
        api_key = (_ST_SECRETS.get('ANTHROPIC_API_KEY') if _ST_SECRETS else os.getenv('ANTHROPIC_API_KEY'))
        if api_key and api_key != 'your-api-key-here':
            self.claude_client = anthropic.Anthropic(api_key=api_key)
            print("   âœ… Claude API å·²å¾ secrets/.env è¼‰å…¥")
        else:
            print("   âš ï¸ æœªè¨­å®šæœ‰æ•ˆçš„ ANTHROPIC_API_KEYï¼Œå°‡ä½¿ç”¨æœ¬åœ°åŒ¹é…ç®—æ³•")
            print("   ğŸ’¡ è«‹åœ¨ Streamlit Secrets æˆ– .env æª”æ¡ˆè¨­å®šä½ çš„ Anthropic API Key")
    
    def transcribe_audio_with_timestamps(self):
        """ä½¿ç”¨ Whisper è½‰æ›èªéŸ³ç‚ºæ–‡å­—ï¼ŒåŒ…å«æ™‚é–“æˆ³"""
        print("ğŸ¤ åˆ†æèªéŸ³å…§å®¹...")
        
        # ä½¿ç”¨ Whisper è½‰éŒ„éŸ³é »
        result = self.whisper_model.transcribe(
            self.audio_path,
            language='zh',  # å¯ä»¥æ”¹ç‚º 'en' æˆ– None (è‡ªå‹•æª¢æ¸¬)
            word_timestamps=True
        )
        
        # æ•´ç†è½‰éŒ„çµæœ
        segments = []
        for segment in result['segments']:
            segments.append({
                'start': segment['start'],
                'end': segment['end'],
                'text': segment['text'].strip(),
                'words': segment.get('words', [])
            })
        
        total_duration = result.get('duration', librosa.get_duration(filename=self.audio_path))
        
        print(f"   âœ… èªéŸ³è½‰éŒ„å®Œæˆï¼Œå…± {len(segments)} å€‹ç‰‡æ®µ")
        print(f"   ğŸ“ è½‰éŒ„å…§å®¹é è¦½: {result['text'][:100]}...")
        
        return {
            'segments': segments,
            'full_text': result['text'],
            'duration': total_duration
        }
    
    def extract_text_from_slides(self):
        """ä½¿ç”¨ OCR æå–ç°¡å ±æ–‡å­—"""
        print("ğŸ” åˆ†æç°¡å ±å…§å®¹...")
        
        image_extensions = ["*.jpg", "*.jpeg", "*.png", "*.JPG", "*.JPEG", "*.PNG"]
        slides = []
        for ext in image_extensions:
            slides.extend(self.slides_folder.glob(ext))
        
        slides.sort(key=lambda x: x.name)
        
        slides_content = []
        for i, slide_path in enumerate(slides):
            print(f"   ğŸ“„ åˆ†æç°¡å ± {i+1}/{len(slides)}: {slide_path.name}")
            
            try:
                # ä½¿ç”¨ EasyOCR æå–æ–‡å­—
                result = self.ocr_reader.readtext(str(slide_path))
                
                # æ•´ç†æå–çš„æ–‡å­—
                extracted_text = []
                for (bbox, text, confidence) in result:
                    if confidence > 0.5:  # åªä¿ç•™ä¿¡å¿ƒåº¦é«˜çš„æ–‡å­—
                        extracted_text.append(text.strip())
                
                slide_text = ' '.join(extracted_text)
                
                slides_content.append({
                    'slide_index': i,
                    'slide_path': slide_path,
                    'slide_name': slide_path.name,
                    'extracted_text': slide_text,
                    'word_count': len(slide_text.split()) if slide_text else 0
                })
                
                print(f"      æå–æ–‡å­—: {slide_text[:80]}..." if slide_text else "      æœªæª¢æ¸¬åˆ°æ–‡å­—")
                
            except Exception as e:
                print(f"      âš ï¸ è™•ç†å¤±æ•—: {e}")
                slides_content.append({
                    'slide_index': i,
                    'slide_path': slide_path,
                    'slide_name': slide_path.name,
                    'extracted_text': '',
                    'word_count': 0
                })
        
        print(f"   âœ… ç°¡å ±åˆ†æå®Œæˆï¼Œå…± {len(slides_content)} å¼µ")
        return slides_content
    
    def ai_content_matching(self, speech_data, slides_data):
        """ä½¿ç”¨ AI åˆ†æèªéŸ³å’Œç°¡å ±å…§å®¹çš„åŒ¹é…é—œä¿‚"""
        print("ğŸ¤– AI å…§å®¹åŒ¹é…åˆ†æ...")
        
        if not self.claude_client:
            return self.fallback_content_matching(speech_data, slides_data)
        
        try:
            # æº–å‚™æç¤ºè©
            slides_info = []
            for slide in slides_data:
                slides_info.append({
                    'index': slide['slide_index'],
                    'name': slide['slide_name'],
                    'content': slide['extracted_text'][:200]  # é™åˆ¶é•·åº¦
                })
            
            speech_segments = []
            for segment in speech_data['segments']:
                speech_segments.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text']
                })
            
            user_prompt = f"""
                ç°¡å ±å…§å®¹:
                {json.dumps(slides_info, ensure_ascii=False, indent=2)}

                èªéŸ³å…§å®¹:
                {json.dumps(speech_segments, ensure_ascii=False, indent=2)}
                """
            system_prompt = """
                ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„èª²ç¨‹å½±ç‰‡è£½ä½œå°å¹«æ‰‹ã€‚
                é€™å€‹ç°¡å ±æ˜¯æŒ‰é †åºè¨­è¨ˆçš„ï¼Œæ¯å¼µç°¡å ±å°æ‡‰ä¸€å€‹æ®µè½çš„èªéŸ³å…§å®¹ï¼Œè«‹è©³ç´°åœ°åˆ†æèªéŸ³å’Œç°¡å ±çš„å…§å®¹ï¼Œåˆ¤æ–·ä»€éº¼æ™‚å€™æ‡‰è©²ä½¿ç”¨å“ªå¼µç°¡å ±ã€‚

                ## åˆ‡æ›åŸå‰‡:
                - ç°¡å ±å¿…é ˆæŒ‰é †åºæ’­æ”¾ (0â†’1â†’2â†’3...)ï¼Œä¸å¯ä»¥è·³éæˆ–é‡è¤‡ã€‚
                - åªæœ‰ç•¶èªéŸ³å…§å®¹æ˜ç¢ºé–‹å§‹è¨è«–ä¸‹ä¸€å¼µç°¡å ±çš„ä¸»é¡Œæ™‚ï¼Œæ‰åˆ‡æ›
                - å¦‚æœèªéŸ³é‚„åœ¨å»¶çºŒç•¶å‰ç°¡å ±çš„ä¸»é¡Œï¼Œå°±ä¸è¦åˆ‡æ›
                - å°‹æ‰¾æ˜ç¢ºçš„ç« ç¯€æ¨™é¡Œã€ç·¨è™Ÿæˆ–ä¸»é¡Œè½‰æ›æ™‚æ©Ÿ
                - æœ€å¾Œä¸€å¼µç°¡å ±æŒçºŒåˆ°éŸ³é »çµæŸ
                - æ¯å¼µç°¡å ±æ‡‰è©²æœ‰è¶³å¤ çš„æ™‚é–“é¡¯ç¤ºï¼Œä¸è¦éçŸ­

                ## åˆ‡æ›æ™‚æ©ŸæŒ‡æ¨™:
                - è½åˆ°æ–°çš„ä¸»é¡Œæ¨™é¡Œ
                - èªéŸ³å…§å®¹èˆ‡ä¸‹ä¸€å¼µç°¡å ±çš„æ¨™é¡Œç¬¦åˆ
                - ç•¶å‰ä¸»é¡Œæ˜é¡¯çµæŸï¼Œé–‹å§‹è¨è«–å…¨æ–°çš„æ¦‚å¿µ

                ## è¼¸å‡ºæ ¼å¼:
                è¼¸å‡ºæ ¼å¼ç‚º JSONï¼ŒåŒ…å«æ¯å¼µç°¡å ±çš„æ™‚é–“ç¯„åœ:
                {{
                "slide_timings": [
                    {{
                        "slide_index": 1,
                        "start_time": 60.0,
                        "end_time": 120.0,
                        "reason": "æ˜ç¢ºè½åˆ°ç›¸ä¼¼çš„è§€å¿µï¼Œä¸”å…§å®¹å®Œå…¨åŒ¹é…ç°¡å ±1"
                    }}
                ]
                }}
                """

            response = self.claude_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=10000,
                temperature=0.3,
                system=system_prompt,
                messages=[
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            # è§£æ AI å›æ‡‰
            ai_response = response.content[0].text
            print(f"   ğŸ” Claude å®Œæ•´å›æ‡‰:")
            print("=" * 80)
            print(ai_response)
            print("=" * 80)
            
            # æå– JSON å…§å®¹ï¼ˆè™•ç† markdown ä»£ç¢¼å¡Šæ ¼å¼ï¼‰
            import re
            
            # å˜—è©¦å¾ markdown ä»£ç¢¼å¡Šä¸­æå– JSON
            json_match = re.search(r'```json\s*(.*?)\s*```', ai_response, re.DOTALL)
            if json_match:
                json_content = json_match.group(1)
            else:
                # å¦‚æœæ²’æœ‰ markdown æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å›æ‡‰
                json_content = ai_response
            
            print(f"   ğŸ“„ æå–çš„ JSON å…§å®¹:")
            print("-" * 60)
            print(json_content)
            print("-" * 60)
            
            # å˜—è©¦è§£æ JSON
            try:
                timing_data = json.loads(json_content)
                slide_timings = timing_data.get('slide_timings', [])
                
                # è½‰æ›ç‚ºåŸä¾†çš„ matches æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
                matches = []
                total_duration = speech_data['duration']
                
                for timing in slide_timings:
                    slide_index = timing['slide_index']
                    start_time = timing['start_time']
                    end_time = min(timing['end_time'], total_duration)  # ç¢ºä¿ä¸è¶…ééŸ³é »é•·åº¦
                    
                    # æ‰¾åˆ°é€™å€‹æ™‚é–“ç¯„åœå…§çš„èªéŸ³ç‰‡æ®µ
                    segments_in_range = [
                        seg for seg in speech_data['segments']
                        if seg['start'] >= start_time and seg['end'] <= end_time
                    ]
                    
                    if segments_in_range:
                        # åˆä½µé€™å€‹æ™‚é–“ç¯„åœå…§çš„æ‰€æœ‰èªéŸ³æ–‡æœ¬
                        combined_text = ' '.join([seg['text'] for seg in segments_in_range])
                    else:
                        combined_text = f"æ™‚é–“ç¯„åœ {start_time:.1f}s - {end_time:.1f}s"
                    
                    matches.append({
                        'segment_start': start_time,
                        'segment_end': end_time,
                        'segment_text': combined_text,
                        'recommended_slide': slide_index,
                        'confidence': 0.9,
                        'reason': timing.get('reason', 'æŒ‰é †åºæ’­æ”¾ç°¡å ±')
                    })
                
                print(f"   âœ… AI æ™‚é–“è»¸åˆ†æå®Œæˆï¼Œç”Ÿæˆ {len(matches)} å€‹ç°¡å ±æ®µ")
                return matches
                
            except json.JSONDecodeError as e:
                print(f"   âŒ JSON è§£æéŒ¯èª¤: {e}")
                print(f"   ğŸ“„ æå–çš„ JSON å…§å®¹:\n{json_content[:1000]}...")
                # ä¸ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼Œè€Œæ˜¯æ‹‹å‡ºéŒ¯èª¤
                raise Exception(f"Claude API å›æ‡‰æ ¼å¼éŒ¯èª¤: {e}")
                
        except Exception as e:
            print(f"   âŒ AI åŒ¹é…å¤±æ•—: {e}")
            # ä¸ä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆï¼Œç›´æ¥æ‹‹å‡ºéŒ¯èª¤
            raise e
    
    def fallback_content_matching(self, speech_data, slides_data):
        """å‚™ç”¨çš„ç°¡å–®åŒ¹é…ç®—æ³•"""
        print("   ğŸ”„ ä½¿ç”¨åŸºæ–¼æ™‚é–“çš„ç°¡å–®åŒ¹é…ç®—æ³•")
        
        segments = speech_data['segments']
        slides = slides_data
        
        matches = []
        total_duration = speech_data['duration']
        
        if not segments or not slides:
            return matches
        
        # ç°¡å–®ç­–ç•¥ï¼šæ ¹æ“šæ™‚é–“å‡å‹»åˆ†é…ç°¡å ±
        for i, segment in enumerate(segments):
            # æ ¹æ“šæ™‚é–“æ¯”ä¾‹é¸æ“‡ç°¡å ±
            progress = segment['start'] / total_duration
            slide_index = min(int(progress * len(slides)), len(slides) - 1)
            
            matches.append({
                'segment_start': segment['start'],
                'segment_end': segment['end'],
                'segment_text': segment['text'],
                'recommended_slide': slide_index,
                'confidence': 0.6,
                'reason': 'åŸºæ–¼æ™‚é–“é †åºçš„è‡ªå‹•åŒ¹é…'
            })
        
        return matches
    
    def create_timeline_from_matches(self, matches, slides_data):
        """æ ¹æ“šåŒ¹é…çµæœå»ºç«‹å½±ç‰‡æ™‚é–“è»¸"""
        print("â° ç”Ÿæˆå½±ç‰‡æ™‚é–“è»¸...")
        
        timeline = []
        
        for match in matches:
            slide_index = match['recommended_slide']
            
            if 0 <= slide_index < len(slides_data):
                slide_info = slides_data[slide_index]
                
                timeline.append({
                    'start_time': match['segment_start'],
                    'end_time': match['segment_end'],
                    'duration': match['segment_end'] - match['segment_start'],
                    'slide_path': slide_info['slide_path'],
                    'slide_name': slide_info['slide_name'],
                    'speech_text': match['segment_text'],
                    'confidence': match['confidence']
                })
        
        # åˆä½µç›¸åŒç°¡å ±çš„é€£çºŒç‰‡æ®µ
        merged_timeline = self.merge_consecutive_slides(timeline)
        
        print(f"   âœ… æ™‚é–“è»¸ç”Ÿæˆå®Œæˆï¼Œå…± {len(merged_timeline)} å€‹ç‰‡æ®µ")
        return merged_timeline
    
    def merge_consecutive_slides(self, timeline):
        """åˆä½µä½¿ç”¨ç›¸åŒç°¡å ±çš„é€£çºŒç‰‡æ®µ"""
        if not timeline:
            return []
        
        merged = []
        current_segment = timeline[0].copy()
        
        for i in range(1, len(timeline)):
            next_segment = timeline[i]
            
            # å¦‚æœæ˜¯ç›¸åŒçš„ç°¡å ±ä¸”æ™‚é–“é€£çºŒï¼Œå°±åˆä½µ
            if (current_segment['slide_path'] == next_segment['slide_path'] and 
                abs(current_segment['end_time'] - next_segment['start_time']) < 1.0):
                
                # åˆä½µç‰‡æ®µ
                current_segment['end_time'] = next_segment['end_time']
                current_segment['duration'] = current_segment['end_time'] - current_segment['start_time']
                current_segment['speech_text'] += ' ' + next_segment['speech_text']
                
            else:
                # ä¸åŒç°¡å ±ï¼ŒåŠ å…¥å‰ä¸€å€‹ç‰‡æ®µä¸¦é–‹å§‹æ–°çš„
                merged.append(current_segment)
                current_segment = next_segment.copy()
        
        # åŠ å…¥æœ€å¾Œä¸€å€‹ç‰‡æ®µ
        merged.append(current_segment)
        
        return merged
    
    def create_video_clips(self, timeline):
        """æ ¹æ“šæ™‚é–“è»¸å»ºç«‹å½±ç‰‡ç‰‡æ®µ"""
        print("ğŸ¥ ç”Ÿæˆå½±ç‰‡ç‰‡æ®µ...")
        
        clips = []
        
        for i, segment in enumerate(timeline):
            try:
                slide_path = segment['slide_path']
                start_time = segment['start_time']
                duration = segment['duration']
                
                # ç¢ºä¿æœ€çŸ­æŒçºŒæ™‚é–“
                duration = max(duration, 1.0)
                
                # å»ºç«‹åœ–ç‰‡ç‰‡æ®µ
                img_clip = ImageClip(str(slide_path), duration=duration)
                img_clip = img_clip.resized(height=720)
                img_clip = img_clip.with_start(start_time)
                
                clips.append(img_clip)
                
                print(f"   ğŸ“„ ç‰‡æ®µ {i+1}: {slide_path.name} ({duration:.1f}s) - {segment['speech_text'][:50]}...")
                
            except Exception as e:
                print(f"   âš ï¸ è™•ç†ç‰‡æ®µ {i+1} æ™‚å‡ºéŒ¯: {e}")
                continue
        
        print(f"   âœ… æˆåŠŸå»ºç«‹ {len(clips)} å€‹å½±ç‰‡ç‰‡æ®µ")
        return clips
    
    def generate_smart_video(self):
        """ç”Ÿæˆæ™ºæ…§èª²ç¨‹å½±ç‰‡"""
        print("ğŸš€ é–‹å§‹ AI æ™ºæ…§å½±ç‰‡ç”Ÿæˆ...")
        print("=" * 60)
        
        # 1. èªéŸ³è½‰æ–‡å­—
        speech_data = self.transcribe_audio_with_timestamps()
        
        # 2. ç°¡å ±æ–‡å­—æå–
        slides_data = self.extract_text_from_slides()
        
        if not slides_data:
            print("âŒ æ²’æœ‰æ‰¾åˆ°ç°¡å ±ï¼")
            return
        
        # 3. AI å…§å®¹åŒ¹é…
        matches = self.ai_content_matching(speech_data, slides_data)
        
        if not matches:
            print("âŒ ç„¡æ³•ç”Ÿæˆå…§å®¹åŒ¹é…ï¼")
            return
        
        # 4. å»ºç«‹æ™‚é–“è»¸
        timeline = self.create_timeline_from_matches(matches, slides_data)
        
        # 5. ç”Ÿæˆå½±ç‰‡ç‰‡æ®µ
        video_clips = self.create_video_clips(timeline)
        
        if not video_clips:
            print("âŒ ç„¡æ³•å»ºç«‹å½±ç‰‡ç‰‡æ®µï¼")
            return
        
        # 6. åˆæˆæœ€çµ‚å½±ç‰‡
        print("ğŸ¬ åˆä½µå½±ç‰‡...")
        audio_clip = AudioFileClip(self.audio_path)
        
        # ç¢ºä¿å½±ç‰‡æ™‚é•·ä¸è¶…ééŸ³é »æ™‚é•·
        video_duration = min(max([clip.end for clip in video_clips]), audio_clip.duration)
        
        final_video = CompositeVideoClip(video_clips, size=(1280, 720))
        final_video = final_video.with_audio(audio_clip)
        final_video = final_video.with_duration(video_duration)
        
        # 7. è¼¸å‡ºå½±ç‰‡
        print(f"ğŸ’¾ æ­£åœ¨å„²å­˜å½±ç‰‡åˆ° {self.output_path}...")
        final_video.write_videofile(
            self.output_path,
            fps=self.fps,
            codec='libx264',
            audio_codec='aac'
        )
        
        print("âœ… AI æ™ºæ…§å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
        
        # 8. ç”ŸæˆåŒ¹é…å ±å‘Š
        self.generate_matching_report(matches, timeline)
        
        # æ¸…ç†è¨˜æ†¶é«”
        final_video.close()
        audio_clip.close()
        for clip in video_clips:
            clip.close()
    
    def generate_matching_report(self, matches, timeline):
        """ç”Ÿæˆå…§å®¹åŒ¹é…å ±å‘Š"""
        # å»ºç«‹ logs è³‡æ–™å¤¾ï¼ˆå¦‚ä¸å­˜åœ¨ï¼‰
        logs_dir = Path("logs")
        logs_dir.mkdir(parents=True, exist_ok=True)
        # ç”¢ç”Ÿæ™‚é–“æˆ³æª”å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = logs_dir / f"matching_report_{timestamp}.json"
        
        report = {
            'generation_time': datetime.now().isoformat(),
            'total_matches': len(matches),
            'total_timeline_segments': len(timeline),
            'matches': matches,
            'timeline': []
        }
        
        # è½‰æ› timeline ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        for segment in timeline:
            report['timeline'].append({
                'start_time': segment['start_time'],
                'end_time': segment['end_time'],
                'duration': segment['duration'],
                'slide_name': segment['slide_name'],
                'speech_text': segment['speech_text'],
                'confidence': segment['confidence']
            })
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š åŒ¹é…å ±å‘Šå·²å„²å­˜è‡³: {report_path}")

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸ§  AI æ™ºæ…§èª²ç¨‹å½±ç‰‡ç”Ÿæˆç³»çµ±")
    print("=" * 60)
    
    # è¨­å®šæª”æ¡ˆè·¯å¾‘
    audio_path = "audio.mp3"
    slides_folder = "images"
    output_path = "ai_lecture_video.mp4"
    
    # æª¢æŸ¥æª”æ¡ˆ
    if not os.path.exists(audio_path):
        print(f"âŒ æ‰¾ä¸åˆ°éŸ³é »æª”æ¡ˆ: {audio_path}")
        return
    
    if not os.path.exists(slides_folder):
        print(f"âŒ æ‰¾ä¸åˆ°ç°¡å ±è³‡æ–™å¤¾: {slides_folder}")
        return
    
    # æª¢æŸ¥ .env æª”æ¡ˆå’Œ API Key
    if not os.path.exists('.env'):
        print("âš ï¸  æç¤º: æœªæ‰¾åˆ° .env æª”æ¡ˆï¼Œå°‡å»ºç«‹ç¯„ä¾‹æª”æ¡ˆ")
        with open('.env', 'w', encoding='utf-8') as f:
            f.write("# AI æ™ºæ…§èª²ç¨‹å½±ç‰‡ç”Ÿæˆç³»çµ±è¨­å®šæª”\n")
            f.write("# è«‹å°‡ your-api-key-here æ›¿æ›ç‚ºä½ çš„å¯¦éš› Anthropic API Key\n\n")
            f.write("ANTHROPIC_API_KEY=your-api-key-here\n")
            f.write("\n# å¯é¸è¨­å®š\n")
            f.write("# WHISPER_MODEL=base\n")
            f.write("# OCR_LANGUAGES=ch_tra,en\n")
            f.write("# VIDEO_FPS=25\n")
        print("   âœ… å·²å»ºç«‹ .env æª”æ¡ˆï¼Œè«‹ç·¨è¼¯ä¸¦è¨­å®šä½ çš„ API Key")
    
    api_key = os.getenv('ANTHROPIC_API_KEY')
    if not api_key or api_key == 'your-api-key-here':
        print("âš ï¸  æç¤º: è«‹åœ¨ .env æª”æ¡ˆä¸­è¨­å®šæœ‰æ•ˆçš„ ANTHROPIC_API_KEY")
        print("   ç·¨è¼¯ .env æª”æ¡ˆä¸¦å°‡ 'your-api-key-here' æ›¿æ›ç‚ºå¯¦éš›çš„ API Key")
    
    # å»ºç«‹ AI å½±ç‰‡ç”Ÿæˆå™¨
    creator = AILectureCreator(audio_path, slides_folder, output_path)
    
    # ç”Ÿæˆå½±ç‰‡
    try:
        creator.generate_smart_video()
        print(f"\nğŸ‰ æˆåŠŸï¼ä½ çš„ AI æ™ºæ…§å½±ç‰‡å·²å„²å­˜ç‚º: {output_path}")
        print("\nğŸš€ ç³»çµ±ç‰¹è‰²:")
        print("   â€¢ ä½¿ç”¨ Whisper é€²è¡Œç²¾ç¢ºèªéŸ³è­˜åˆ¥")
        print("   â€¢ ä½¿ç”¨ OCR æå–ç°¡å ±æ–‡å­—å…§å®¹")
        print("   â€¢ ä½¿ç”¨ Claude AI åˆ†æèªæ„ç›¸é—œæ€§é€²è¡Œæ™ºæ…§åŒ¹é…")
        print("   â€¢ è‡ªå‹•åˆä½µé€£çºŒç›¸åŒç°¡å ±ç‰‡æ®µ")
        print("   â€¢ ç”Ÿæˆè©³ç´°çš„åŒ¹é…åˆ†æå ±å‘Š")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå½±ç‰‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 